{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4762fdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# Cell 0 â€” install timm and mount drive (run if needed)\n",
    "!pip install timm --quiet\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719e64a7",
   "metadata": {},
   "source": [
    "# 1.**Import Most Needed Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e6abcdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a:\\PyTorch\\Environment\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Cell 1 â€” imports and constants\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "import timm\n",
    "\n",
    "# Path to your dataset root in Drive (adjust if different)\n",
    "DATASET_ROOT = r\"A:\\PyTorch\\Project\\140k_Real_Fake_Faces\\real_vs_fake\\real-vs-fake\"  # <-- edit if needed\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a41a81",
   "metadata": {},
   "source": [
    "# 2.**DeepfakeDataset class**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae373604",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2 â€” DeepfakeDataset class\n",
    "class DeepfakeDataset(Dataset):\n",
    "    def __init__(self, root, split=\"train\", transform=None):\n",
    "        self.root = os.path.join(root, split)\n",
    "        self.transform = transform\n",
    "        self.samples = []\n",
    "        for label_name, label_value in [(\"real\", 0), (\"fake\", 1)]:\n",
    "            folder = os.path.join(self.root, label_name)\n",
    "            if not os.path.isdir(folder):\n",
    "                continue\n",
    "            for filename in os.listdir(folder):\n",
    "                fpath = os.path.join(folder, filename)\n",
    "                if os.path.isfile(fpath):\n",
    "                    self.samples.append((fpath, label_value))\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    def __getitem__(self, idx):\n",
    "        path, label = self.samples[idx]\n",
    "        img = Image.open(path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, torch.tensor([label], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c7c7157",
   "metadata": {},
   "source": [
    "# 3.**Transforms for Xception**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cdd2aaeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3 â€” transforms for Xception\n",
    "train_transform = T.Compose([\n",
    "    T.Resize((342, 342)),\n",
    "    T.RandomResizedCrop(299, scale=(0.65, 1.0)),   # crop and resize to 299x299\n",
    "    T.RandomHorizontalFlip(p=0.5),\n",
    "    T.RandomApply([T.ColorJitter(0.4,0.4,0.2,0.1)], p=0.7),   # brightness, contrast, saturation, hue\n",
    "    T.RandomApply([T.GaussianBlur(kernel_size=5)], p=0.3),    # blur with a 5x5 kernel matrix\n",
    "    T.RandomApply([T.RandomPosterize(bits=4)], p=0.2),        # reduce to 4 bits for each channel\n",
    "    T.RandomGrayscale(p=0.02),                    # convert to grayscale with 2% probability\n",
    "    T.ToTensor(),\n",
    "    T.Normalize([0.5]*3, [0.5]*3),             # normalize to [-1, 1]\n",
    "])\n",
    "\n",
    "val_transform = T.Compose([\n",
    "    T.Resize((299, 299)),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize([0.5]*3, [0.5]*3),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1cec1c",
   "metadata": {},
   "source": [
    "# 4.**Create Datasets & Dataloaders**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ed270fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset sizes â€” train: 100000 val: 20000 test: 20000\n"
     ]
    }
   ],
   "source": [
    "# Cell 4 â€” create datasets & dataloaders\n",
    "batch_size = 32   # reduce to 16 if OOM\n",
    "num_workers = 4\n",
    "\n",
    "train_ds = DeepfakeDataset(DATASET_ROOT, \"train\", train_transform)\n",
    "val_ds   = DeepfakeDataset(DATASET_ROOT, \"val\", val_transform)\n",
    "test_ds  = DeepfakeDataset(DATASET_ROOT, \"test\", val_transform)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "val_loader   = DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "test_loader  = DataLoader(test_ds, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "\n",
    "print(\"Dataset sizes â€” train:\", len(train_ds), \"val:\", len(val_ds), \"test:\", len(test_ds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797003ee",
   "metadata": {},
   "source": [
    "# 5.**Xception model with dropout head**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f33191",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5 â€” Xception model with dropout head\n",
    "def get_xception_model(dropout=0.3):\n",
    "    base = timm.create_model(\"xception\", pretrained=True)\n",
    "    # Detect which classification layer exists and replace it\n",
    "    if hasattr(base, \"fc\"):\n",
    "        in_features = base.fc.in_features\n",
    "    elif hasattr(base, \"classifier\"):\n",
    "        in_features = base.classifier.in_features\n",
    "    else:\n",
    "        raise RuntimeError(\"Unknown xception head name\")\n",
    "    base.fc = nn.Sequential(\n",
    "        nn.Dropout(p=dropout),\n",
    "        nn.Linear(in_features, 1)\n",
    "    )\n",
    "    return base\n",
    "\n",
    "model = get_xception_model().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9126a4",
   "metadata": {},
   "source": [
    "# 6.**Loss, Optimizer, Scheduler, Scaler**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac16ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6 â€” loss, optimizer, scheduler, scaler\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# param groups\n",
    "head_params = [p for n,p in model.named_parameters() if \"fc\" in n]\n",
    "backbone_params = [p for n,p in model.named_parameters() if \"fc\" not in n]\n",
    "\n",
    "optimizer = torch.optim.AdamW([\n",
    "    {\"params\": backbone_params, \"lr\": 1e-5, \"weight_decay\": 1e-4},\n",
    "    {\"params\": head_params,     \"lr\": 1e-4, \"weight_decay\": 1e-3},\n",
    "])\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode=\"max\", factor=0.5, patience=2, verbose=True, min_lr=1e-7\n",
    ")\n",
    "\n",
    "scaler = torch.cuda.amp.GradScaler()   # for mixed precision training\n",
    "print(\"Optimizer and scheduler created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1daf6bf",
   "metadata": {},
   "source": [
    "# 7.**Checkpoint Helpers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d069fada",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7 â€” checkpoint helpers\n",
    "CHECKPOINT_DIR = r\"A:\\PyTorch\"\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "\n",
    "def save_checkpoint(epoch, model, optimizer, scaler, scheduler, best_val_acc, filename):\n",
    "    checkpoint = {\n",
    "        \"epoch\": epoch,\n",
    "        \"model_state\": model.state_dict(),\n",
    "        \"optimizer_state\": optimizer.state_dict(),\n",
    "        \"scaler_state\": scaler.state_dict() if scaler is not None else None,\n",
    "        \"scheduler_state\": scheduler.state_dict(),\n",
    "        \"best_val_acc\": best_val_acc,\n",
    "        \"model_name\": model.__class__.__name__,\n",
    "        \"timm_name\": getattr(model, \"default_cfg\", {}).get(\"architecture\", None)\n",
    "    }\n",
    "    torch.save(checkpoint, filename)\n",
    "    print(f\"ðŸ’¾ Saved: {filename}\")\n",
    "\n",
    "def load_checkpoint(path, model, optimizer, scaler, scheduler):\n",
    "    print(f\"ðŸ”„ Loading checkpoint: {path}\")\n",
    "    checkpoint = torch.load(path, map_location=device)\n",
    "    saved_name = checkpoint.get(\"model_name\", None)\n",
    "    current_name = model.__class__.__name__\n",
    "    if saved_name is not None and saved_name != current_name:\n",
    "        print(f\"âš ï¸ WARNING: Checkpoint was trained on: {saved_name}\")\n",
    "        print(f\"âš ï¸ Current model is: {current_name}\")\n",
    "        print(\"âš ï¸ Loading anyway â€” ensure architecture compatibility.\")\n",
    "    \n",
    "    model.load_state_dict(checkpoint[\"model_state\"], strict=True)\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer_state\"])\n",
    "    if checkpoint[\"scaler_state\"] is not None and scaler is not None:\n",
    "        scaler.load_state_dict(checkpoint[\"scaler_state\"])\n",
    "    scheduler.load_state_dict(checkpoint[\"scheduler_state\"])\n",
    "    start_epoch = checkpoint[\"epoch\"] + 1\n",
    "    best_val_acc = checkpoint[\"best_val_acc\"]\n",
    "    print(f\"âœ” Resumed from epoch {start_epoch}, best_val_acc={best_val_acc:.2f}%\")\n",
    "    return start_epoch, best_val_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e231b21c",
   "metadata": {},
   "source": [
    "# 8.**Train & Validate**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a66be229",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8 â€” train & validate\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train_one_epoch(model, train_loader, criterion, optimizer, scaler, device):\n",
    "    model.train()        # set to training mode\n",
    "    running_loss = 0.0   # cumulative loss (batch)\n",
    "    correct = 0          # correct predictions\n",
    "    total = 0            # total samples\n",
    "    loop = tqdm(train_loader, leave=False)  # progress bar\n",
    "    for imgs, labels in loop:\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()   # reset gradients\n",
    "        # AMP new syntax\n",
    "        with torch.amp.autocast(device_type=\"cuda\"):\n",
    "            outputs = model(imgs)\n",
    "            loss = criterion(outputs, labels)  # compute loss\n",
    "        # Backprop with scaling & clipping\n",
    "        scaler.scale(loss).backward()   # scaled backward pass\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)   # gradient clipping\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        running_loss += loss.item() * imgs.size(0)\n",
    "        preds = (torch.sigmoid(outputs) >= 0.5).float()\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "        loop.set_description(f\"loss: {loss.item():.4f}\")\n",
    "    return running_loss / total, correct / total * 100\n",
    "\n",
    "def validate(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in val_loader:\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "            outputs = model(imgs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item() * imgs.size(0)\n",
    "            preds = (torch.sigmoid(outputs) >= 0.5).float()\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    return running_loss / total, correct / total * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30ec000",
   "metadata": {},
   "source": [
    "# 9.**Main Training Loop**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc4fcab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9 â€” main training loop\n",
    "resume = False\n",
    "checkpoint_path = f\"{CHECKPOINT_DIR}/checkpoint_epoch_6.pth\"  # edit if you have a checkpoint\n",
    "\n",
    "if resume:\n",
    "    start_epoch, best_val_acc = load_checkpoint(checkpoint_path, model, optimizer, scaler, scheduler)\n",
    "else:\n",
    "    start_epoch = 0\n",
    "    best_val_acc = 0.0\n",
    "\n",
    "epochs = 8\n",
    "patience = 3\n",
    "no_improve = 0\n",
    "\n",
    "for epoch in range(start_epoch, epochs):\n",
    "    print(f\"\\nðŸ”µ Epoch {epoch+1}/{epochs}\")\n",
    "    train_loss, train_acc = train_one_epoch(model, train_loader, criterion, optimizer, scaler, device)\n",
    "    val_loss, val_acc = validate(model, val_loader, criterion, device)\n",
    "    print(f\"Train Loss: {train_loss:.4f} | Acc: {train_acc:.2f}%\")\n",
    "    print(f\" Val  Loss: {val_loss:.4f} | Acc: {val_acc:.2f}%\")\n",
    "    scheduler.step(val_acc)\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        no_improve = 0\n",
    "        save_checkpoint(epoch, model, optimizer, scaler, scheduler, best_val_acc, f\"{CHECKPOINT_DIR}/best_model.pth\")\n",
    "        print(\"ðŸ† Best model updated!\")\n",
    "    else:\n",
    "        no_improve += 1\n",
    "    if no_improve >= patience:\n",
    "        print(\"â›” Early stopping triggered!\")\n",
    "        break\n",
    "    save_checkpoint(epoch, model, optimizer, scaler, scheduler, best_val_acc, f\"{CHECKPOINT_DIR}/checkpoint_epoch_{epoch+1}.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8339b0c6",
   "metadata": {},
   "source": [
    "# 10.**Evaluation & Single-Image Prediction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3efc4656",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10 â€” evaluation & single-image prediction\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "# load best model\n",
    "best_model_path = f\"{CHECKPOINT_DIR}/best_model.pth\"\n",
    "checkpoint = torch.load(best_model_path, map_location=device)\n",
    "model.load_state_dict(checkpoint[\"model_state\"])\n",
    "model.to(device).eval()\n",
    "\n",
    "def get_preds_probs(model, loader):\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in loader:\n",
    "            imgs = imgs.to(device)\n",
    "            out = model(imgs)\n",
    "            probs = torch.sigmoid(out).squeeze().cpu().tolist()\n",
    "            # ensure list if single item\n",
    "            if isinstance(probs, float):\n",
    "                probs = [probs]\n",
    "            all_probs.extend(probs)\n",
    "            all_labels.extend(labels.squeeze().tolist())\n",
    "    return all_labels, all_probs\n",
    "\n",
    "y_true, y_probs = get_preds_probs(model, test_loader)\n",
    "y_pred = [1 if p >= 0.5 else 0 for p in y_probs]\n",
    "\n",
    "accuracy = sum([yt == yp for yt, yp in zip(y_true, y_pred)]) / len(y_true) * 100\n",
    "print(\"Test Accuracy:\", accuracy)\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_true, y_pred))\n",
    "print(classification_report(y_true, y_pred, target_names=[\"real\",\"fake\"]))\n",
    "print(\"ROC AUC:\", roc_auc_score(y_true, y_probs))\n",
    "\n",
    "def predict_image(model, image_path, transform):\n",
    "    img = Image.open(image_path).convert(\"RGB\")\n",
    "    x = transform(img).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        out = model(x)\n",
    "        p = torch.sigmoid(out).item()\n",
    "    return p"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5601ad27",
   "metadata": {},
   "source": [
    "# 11.**Utilities**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5ede5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11 â€” utilities\n",
    "# Example: download best model to local (if needed)\n",
    "# from google.colab import files\n",
    "# files.download(best_model_path)\n",
    "\n",
    "# Quick test on random batch\n",
    "imgs, labels = next(iter(val_loader))\n",
    "imgs, labels = imgs.to(device), labels.to(device)\n",
    "with torch.no_grad():\n",
    "    out = model(imgs[:4])\n",
    "    print(\"sample outputs:\", torch.sigmoid(out).squeeze().cpu().numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
