{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ca5d656c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available: True\n",
      "Running on GPU: Quadro P2000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"CUDA Available:\", torch.cuda.is_available())\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Running on GPU:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print(\"Running on CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5fd4ce6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "\n",
    "train_ratio = 0.8\n",
    "val_ratio = 0.1\n",
    "test_ratio = 0.1\n",
    "\n",
    "random.seed(42)\n",
    "DATASET_ROOT = r\"A:\\PyTorch\\Project\\140k_Real_Fake_Faces\\real_vs_fake\\real-vs-fake\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7b7e9a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================\n",
    "# âœ“ SWINV2 TINY â€” FULL WORKING VERSION\n",
    "# =========================================\n",
    "\n",
    "# Basic imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "import os\n",
    "import timm\n",
    "\n",
    "\n",
    "# Dataset class\n",
    "class DeepfakeDataset(Dataset):\n",
    "    def __init__(self, root, split=\"train\", transform=None):\n",
    "        self.root = os.path.join(root, split)\n",
    "        self.transform = transform\n",
    "        self.samples = []\n",
    "\n",
    "        for label_name, label_value in [(\"real\", 0), (\"fake\", 1)]:\n",
    "            folder = os.path.join(self.root, label_name)\n",
    "            for filename in os.listdir(folder):\n",
    "                fpath = os.path.join(folder, filename)\n",
    "                if os.path.isfile(fpath):\n",
    "                    self.samples.append((fpath, label_value))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path, label = self.samples[idx]\n",
    "        img = Image.open(path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, torch.tensor([label], dtype=torch.float32)\n",
    "\n",
    "\n",
    "# =========================================\n",
    "# âœ“ TRANSFORMS (SwinV2 uses 256x256)\n",
    "# =========================================\n",
    "train_transform = T.Compose([\n",
    "    T.Resize((280, 280)),\n",
    "    T.RandomResizedCrop(256, scale=(0.7, 1.0)),\n",
    "    T.RandomHorizontalFlip(0.5),\n",
    "    T.RandomApply([T.ColorJitter(0.3, 0.3, 0.2, 0.1)], p=0.7),\n",
    "    T.RandomApply([T.GaussianBlur(kernel_size=3)], p=0.3),\n",
    "    T.RandomGrayscale(p=0.02),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize([0.5]*3, [0.5]*3),\n",
    "])\n",
    "\n",
    "val_transform = T.Compose([\n",
    "    T.Resize((256, 256)),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize([0.5]*3, [0.5]*3),\n",
    "])\n",
    "\n",
    "\n",
    "# =========================================\n",
    "# âœ“ DATALOADERS\n",
    "# =========================================\n",
    "train_ds = DeepfakeDataset(DATASET_ROOT, \"train\", train_transform)\n",
    "val_ds   = DeepfakeDataset(DATASET_ROOT, \"val\", val_transform)\n",
    "test_ds  = DeepfakeDataset(DATASET_ROOT, \"test\", val_transform)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=32, shuffle=True)\n",
    "val_loader   = DataLoader(val_ds, batch_size=32, shuffle=False)\n",
    "test_loader  = DataLoader(test_ds, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "43c8f3da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a:\\PyTorch\\Environment\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\ahmed\\.cache\\huggingface\\hub\\models--timm--swinv2_tiny_window8_256.ms_in1k. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ” SwinV2 Tiny fully initialized!\n",
      "âœ” Optimizer & Scheduler ready!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ahmed\\AppData\\Local\\Temp\\ipykernel_1348\\957326209.py:68: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()\n"
     ]
    }
   ],
   "source": [
    "# =========================================\n",
    "# âœ“ FIXED SWINV2 TINY MODEL\n",
    "# =========================================\n",
    "def get_swin_model(dropout=0.3):\n",
    "    # Load SwinV2 Tiny WITHOUT classifier\n",
    "    model = timm.create_model(\n",
    "        \"swinv2_tiny_window8_256.ms_in1k\",\n",
    "        pretrained=True,\n",
    "        num_classes=0,\n",
    "        features_only=False\n",
    "    )\n",
    "\n",
    "    # -------------------------------------------------\n",
    "    # ðŸ” AUTO-DETECT FEATURE SIZE\n",
    "    # -------------------------------------------------\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        dummy = torch.randn(1, 3, 256, 256)\n",
    "        feats = model.forward_features(dummy)  # raw features\n",
    "        # feats shape likely: (1, C, H, W) OR (1, N, C)\n",
    "        if feats.ndim == 4:   # (B, C, H, W)\n",
    "            feats = nn.AdaptiveAvgPool2d(1)(feats).flatten(1)\n",
    "        elif feats.ndim == 3: # (B, N, C)\n",
    "            feats = feats.mean(dim=1)\n",
    "        feat_dim = feats.shape[1]  # final feature dimension\n",
    "\n",
    "    # -------------------------------------------------\n",
    "    # ðŸ”µ BUILD CLASSIFIER HEAD WITH CORRECT DIM\n",
    "    # -------------------------------------------------\n",
    "    model.head = nn.Sequential(\n",
    "        nn.Dropout(dropout),\n",
    "        nn.Linear(feat_dim, 1)\n",
    "    )\n",
    "\n",
    "    # Wrap forward() to include our head\n",
    "    def forward(self, x):\n",
    "        x = self.forward_features(x)\n",
    "        if x.ndim == 4:\n",
    "            x = nn.AdaptiveAvgPool2d(1)(x).flatten(1)\n",
    "        elif x.ndim == 3:\n",
    "            x = x.mean(dim=1)\n",
    "        x = self.head(x)\n",
    "        return x\n",
    "\n",
    "    model.forward = forward.__get__(model, model.__class__)\n",
    "    return model\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = get_swin_model().to(device)\n",
    "\n",
    "print(\"âœ” SwinV2 Tiny fully initialized!\")\n",
    "\n",
    "\n",
    "# =========================================\n",
    "# âœ“ LOSS, OPTIMIZER, SCHEDULER\n",
    "# =========================================\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "head_params = [p for n, p in model.named_parameters() if \"head\" in n]\n",
    "backbone_params = [p for n, p in model.named_parameters() if \"head\" not in n]\n",
    "\n",
    "optimizer = torch.optim.AdamW([\n",
    "    {\"params\": backbone_params, \"lr\": 1e-5, \"weight_decay\": 1e-4},\n",
    "    {\"params\": head_params,     \"lr\": 1e-4, \"weight_decay\": 1e-3},\n",
    "])\n",
    "\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode=\"max\", factor=0.5, patience=2,\n",
    ")\n",
    "\n",
    "print(\"âœ” Optimizer & Scheduler ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7197918e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    from tqdm import tqdm\n",
    "    loop = tqdm(train_loader, leave=False)\n",
    "\n",
    "    for imgs, labels in loop:\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(imgs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * imgs.size(0)\n",
    "\n",
    "        preds = (torch.sigmoid(outputs) >= 0.5).float()\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "        loop.set_description(f\"loss: {loss.item():.4f}\")\n",
    "\n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = correct / total * 100\n",
    "    return epoch_loss, epoch_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "26b2fdb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "#   CHECKPOINT SAVE / LOAD\n",
    "# ===============================\n",
    "\n",
    "def save_checkpoint(epoch, model, optimizer, scaler, scheduler, best_val_acc, filename):\n",
    "    checkpoint = {\n",
    "        \"epoch\": epoch,\n",
    "        \"model_state\": model.state_dict(),\n",
    "        \"optimizer_state\": optimizer.state_dict(),\n",
    "        \"scaler_state\": scaler.state_dict(),\n",
    "        \"scheduler_state\": scheduler.state_dict(),\n",
    "        \"best_val_acc\": best_val_acc,\n",
    "        \"model_name\": model.__class__.__name__,      # â­ NEW â€” protects against wrong model loading\n",
    "        \"timm_name\": getattr(model, \"default_cfg\", {}).get(\"architecture\", None)  # bonus\n",
    "    }\n",
    "\n",
    "    torch.save(checkpoint, filename)\n",
    "    print(f\"ðŸ’¾ Saved checkpoint to: {filename}\")\n",
    "\n",
    "\n",
    "def load_checkpoint(path, model, optimizer, scaler, scheduler):\n",
    "    print(f\"ðŸ”„ Loading checkpoint: {path}\")\n",
    "\n",
    "    checkpoint = torch.load(path, map_location=device)\n",
    "\n",
    "    # ===============================\n",
    "    #   MODEL MATCHING SAFETY CHECK\n",
    "    # ===============================\n",
    "    saved_name = checkpoint.get(\"model_name\", None)\n",
    "    current_name = model.__class__.__name__\n",
    "\n",
    "    if saved_name is not None and saved_name != current_name:\n",
    "        print(f\"âš ï¸ WARNING: Checkpoint was trained on: {saved_name}\")\n",
    "        print(f\"âš ï¸ Current model is: {current_name}\")\n",
    "        print(\"âš ï¸ Loading anyway, but ensure architectures are compatible.\")\n",
    "\n",
    "    # Load states\n",
    "    model.load_state_dict(checkpoint[\"model_state\"], strict=True)\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer_state\"])\n",
    "    scaler.load_state_dict(checkpoint[\"scaler_state\"])\n",
    "    scheduler.load_state_dict(checkpoint[\"scheduler_state\"])\n",
    "\n",
    "    start_epoch = checkpoint[\"epoch\"] + 1\n",
    "    best_val_acc = checkpoint[\"best_val_acc\"]\n",
    "\n",
    "    print(f\"âœ” Resumed from epoch {start_epoch}\")\n",
    "    print(f\"âœ” Previous best val acc: {best_val_acc:.2f}%\")\n",
    "\n",
    "    return start_epoch, best_val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6f549d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in val_loader:\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(imgs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            running_loss += loss.item() * imgs.size(0)\n",
    "\n",
    "            preds = (torch.sigmoid(outputs) >= 0.5).float()\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc  = correct / total * 100\n",
    "    return epoch_loss, epoch_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8f8a36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”µ Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.6966:   0%|          | 4/3125 [02:17<30:38:25, 35.34s/it]"
     ]
    }
   ],
   "source": [
    "CHECKPOINT_DIR = r\"A:\\PyTorch\"  # ðŸ”µ NEW\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "\n",
    "# ---- OPTIONAL: choose checkpoint to resume ----\n",
    "resume = False\n",
    "checkpoint_path = f\"{CHECKPOINT_DIR}/checkpoint_epoch_8.pth\"\n",
    "\n",
    "if resume:\n",
    "    start_epoch, best_val_acc = load_checkpoint(\n",
    "        checkpoint_path, model, optimizer, scaler, scheduler\n",
    "    )\n",
    "else:\n",
    "    start_epoch = 0\n",
    "    best_val_acc = 0.0\n",
    "\n",
    "epochs = 10   # ðŸ”µ Swin learns slower â†’ use 10 epochs\n",
    "patience = 3\n",
    "no_improve = 0\n",
    "\n",
    "# ---- TRAIN LOOP ----\n",
    "for epoch in range(start_epoch, epochs):\n",
    "    print(f\"\\nðŸ”µ Epoch {epoch+1}/{epochs}\")\n",
    "\n",
    "    train_loss, train_acc = train_one_epoch(\n",
    "    model, train_loader, criterion, optimizer, device\n",
    "    )\n",
    "\n",
    "    val_loss, val_acc = validate(\n",
    "        model, val_loader, criterion, device\n",
    "    )\n",
    "\n",
    "    print(f\"Train Loss: {train_loss:.4f} | Acc: {train_acc:.2f}%\")\n",
    "    print(f\"Val   Loss: {val_loss:.4f} | Acc: {val_acc:.2f}%\")\n",
    "\n",
    "    # Scheduler step\n",
    "    scheduler.step(val_acc)\n",
    "\n",
    "    # ---- BEST MODEL ----\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        no_improve = 0\n",
    "        save_checkpoint(\n",
    "            epoch, model, optimizer, scaler, scheduler,\n",
    "            best_val_acc, f\"{CHECKPOINT_DIR}/best_model.pth\"\n",
    "        )\n",
    "        print(\"ðŸ† Best model updated!\")\n",
    "    else:\n",
    "        no_improve += 1\n",
    "\n",
    "    # ---- EARLY STOPPING ----\n",
    "    if no_improve >= patience:\n",
    "        print(\"â›” Early stopping triggered!\")\n",
    "        break\n",
    "\n",
    "    # ---- EPOCH CHECKPOINT ----\n",
    "    save_checkpoint(\n",
    "        epoch, model, optimizer, scaler, scheduler,\n",
    "        best_val_acc,\n",
    "        f\"{CHECKPOINT_DIR}/checkpoint_epoch_{epoch+1}.pth\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "beb031fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 99.97\n",
      "Confusion Matrix:\n",
      " [[9999    1]\n",
      " [   5 9995]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        real       1.00      1.00      1.00     10000\n",
      "        fake       1.00      1.00      1.00     10000\n",
      "\n",
      "    accuracy                           1.00     20000\n",
      "   macro avg       1.00      1.00      1.00     20000\n",
      "weighted avg       1.00      1.00      1.00     20000\n",
      "\n",
      "ROC AUC: 0.99999953\n"
     ]
    }
   ],
   "source": [
    "# Cell 9 â€” evaluation & single-image prediction (NO NUMPY)\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "# Load best model\n",
    "best_model_path = r\"A:\\PyTorch\\checkpoint_epoch_8.pth\"\n",
    "checkpoint = torch.load(best_model_path, map_location=device)\n",
    "model.load_state_dict(checkpoint[\"model_state\"])\n",
    "model.to(device).eval()\n",
    "\n",
    "# -------------------------------\n",
    "#   Get predictions + probabilities\n",
    "# -------------------------------\n",
    "def get_preds_probs(model, loader):\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in loader:\n",
    "            imgs = imgs.to(device)\n",
    "\n",
    "            outputs = model(imgs)\n",
    "            probs = torch.sigmoid(outputs).squeeze().cpu()   # torch tensor\n",
    "\n",
    "            all_probs.extend(probs.tolist())                  # pure Python list\n",
    "            all_labels.extend(labels.squeeze().tolist())      # pure Python list\n",
    "\n",
    "    return all_labels, all_probs\n",
    "\n",
    "\n",
    "# Run evaluation\n",
    "y_true, y_probs = get_preds_probs(model, test_loader)\n",
    "\n",
    "# Convert to hard predictions\n",
    "y_pred = [1 if p >= 0.5 else 0 for p in y_probs]\n",
    "\n",
    "# -------------------------------\n",
    "#   Metrics\n",
    "# -------------------------------\n",
    "accuracy = sum([yt == yp for yt, yp in zip(y_true, y_pred)]) / len(y_true) * 100\n",
    "print(\"Test Accuracy:\", accuracy)\n",
    "\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_true, y_pred))\n",
    "print(classification_report(y_true, y_pred, target_names=[\"real\", \"fake\"]))\n",
    "\n",
    "# ROC-AUC needs Python lists (not numpy)\n",
    "print(\"ROC AUC:\", roc_auc_score(y_true, y_probs))\n",
    "\n",
    "# -------------------------------\n",
    "#   Single image prediction\n",
    "# -------------------------------\n",
    "def predict_image(model, image_path, transform):\n",
    "    img = Image.open(image_path).convert(\"RGB\")\n",
    "    x = transform(img).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        out = model(x)\n",
    "        p = torch.sigmoid(out).item()\n",
    "\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cca01db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
